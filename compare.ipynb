{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Load and validate Azure AI Services configs\n",
    "AZURE_AI_SERVICES_ENDPOINT = os.getenv(\"AZURE_AI_SERVICES_ENDPOINT\")\n",
    "AZURE_AI_SERVICES_API_VERSION = os.getenv(\"AZURE_AI_SERVICES_API_VERSION\")\n",
    "AZURE_AI_SERVICES_API_KEY = os.getenv(\"AZURE_AI_SERVICES_API_KEY\", None)\n",
    "AZURE_AI_DOCUMENT_ENDPOINT = os.getenv(\"AZURE_AI_DOCUMENT_ENDPOINT\") or os.getenv(\"AZURE_AI_SERVICES_ENDPOINT\")\n",
    "AZURE_AI_DOCUMENT_API_KEY = os.getenv(\"AZURE_AI_DOCUMENT_API_KEY\", None)\n",
    "assert AZURE_AI_SERVICES_ENDPOINT, \"AZURE_AI_SERVICES_ENDPOINT must be set\"\n",
    "assert AZURE_AI_SERVICES_API_VERSION, \"AZURE_AI_SERVICES_API_VERSION must be set\"\n",
    "assert AZURE_AI_DOCUMENT_ENDPOINT, \"AZURE_AI_DOCUMENT_ENDPOINT must be set\"\n",
    "\n",
    "# Load and validate Azure OpenAI configs\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\", None)\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_CHAT_API_VERSION = os.getenv(\"AZURE_OPENAI_CHAT_API_VERSION\")\n",
    "AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_EMBEDDINGS_API_VERSION = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_API_VERSION\")\n",
    "assert AZURE_OPENAI_ENDPOINT, \"AZURE_OPENAI_ENDPOINT must be set\"\n",
    "assert (AZURE_OPENAI_CHAT_DEPLOYMENT_NAME), \"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME must be set\"\n",
    "assert (AZURE_OPENAI_CHAT_API_VERSION), \"AZURE_OPENAI_CHAT_API_VERSION must be set\"\n",
    "assert (AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME), \"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME must be set\"\n",
    "assert (AZURE_OPENAI_EMBEDDINGS_API_VERSION), \"AZURE_OPENAI_EMBEDDINGS_API_VERSION must be set\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "client = AzureChatOpenAI(\n",
    "    model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    openai_api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_CHAT_API_VERSION,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "def load_clauses(jsonl_file):\n",
    "    clauses = []\n",
    "    with open(jsonl_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            if \"clauseName\" in data and \"clauseText\" in data:\n",
    "                clauses.append((data[\"clauseName\"], data[\"clauseText\"]))\n",
    "            else:\n",
    "                print(f\"Skipping line due to missing keys: {line}\")\n",
    "    return clauses\n",
    "\n",
    "def check_clauses(contract_chunks, required_clauses):\n",
    "    clause_similarities = {}\n",
    "    \n",
    "    for clause_name, clause_text in required_clauses:\n",
    "        for chunk in contract_chunks:\n",
    "            prompt = f\"You are a legal assistant who is tasked with ensuring legal documents contain the proper clauses. You will be given two clauses to compare. Your job is to determine if the two clauses express the same intent. The first is the golden clause, the second comes from the document we are reviewing. Golden Clause: {clause_text} and Document Clause: {chunk}. Respond with true if the two clauses have the similar intent. Otherwise, respond with false.\"\n",
    "    \n",
    "            response = client.invoke(prompt)\n",
    "            if response.content.strip().lower() == \"true\":\n",
    "                clause_similarities[clause_name] = True\n",
    "                break\n",
    "        else:\n",
    "            clause_similarities[clause_name] = False\n",
    "    return clause_similarities\n",
    "\n",
    "jsonl_file = \"./master_contract_clause.jsonl\"\n",
    "master_clauses = load_clauses(jsonl_file)\n",
    "\n",
    "sample_chunks = [\n",
    "    \"Borrower confirms that the loan will be repaid before maturity\",\n",
    "    \"The interest starts accruing at signing date.\",\n",
    "    \"All notifications, requests, and correspondences must be sent following the specified procedures.\",\n",
    "    \"All notices, demands, and communications shall be delivered in accordance with the stated procedures.\"\n",
    "]\n",
    "\n",
    "results = check_clauses(sample_chunks, master_clauses)\n",
    "\n",
    "# Print results in Markdown table format\n",
    "print(\"| Clause Name | Intent |\")\n",
    "print(\"|:------------|:-----------|\")\n",
    "for clause_name, similarity in results.items():\n",
    "    print(f\"| {clause_name} | {similarity} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import jaccard_score\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def embed_text(items, model):\n",
    "    \"\"\"\n",
    "    Given a list of strings, returns a NumPy array of sentence embeddings using the SentenceTransformer model.\n",
    "    \"\"\"\n",
    "    if not items:\n",
    "        return np.array([])\n",
    "    return model.encode(items, show_progress_bar=False)\n",
    "\n",
    "def compute_tfidf_similarity(clause_texts, chunk_texts):\n",
    "    \"\"\"\n",
    "    Computes TF-IDF similarity between each clause and each chunk.\n",
    "    Returns a 2D array of shape (num_clauses, num_chunks).\n",
    "    \n",
    "    For simplicity, we do a dot product on the TF-IDF vectors,\n",
    "    then normalize the result into a [0..1] range.\n",
    "    \"\"\"\n",
    "    if not clause_texts or not chunk_texts:\n",
    "        return np.array([])\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(clause_texts + chunk_texts)\n",
    "    \n",
    "    # Partition into clause part and chunk part\n",
    "    clause_tfidf = tfidf_matrix[:len(clause_texts)]\n",
    "    chunk_tfidf = tfidf_matrix[len(clause_texts):]\n",
    "    \n",
    "    # Dot product (sparse matrix multiplication)\n",
    "    similarity_matrix = clause_tfidf * chunk_tfidf.T\n",
    "    similarity_matrix = similarity_matrix.toarray()\n",
    "    \n",
    "    # Normalize [global min..max] -> [0..1]\n",
    "    if similarity_matrix.size > 0 and similarity_matrix.max() != similarity_matrix.min():\n",
    "        similarity_matrix = (\n",
    "            (similarity_matrix - similarity_matrix.min())\n",
    "            / (similarity_matrix.max() - similarity_matrix.min())\n",
    "        )\n",
    "    return similarity_matrix\n",
    "\n",
    "def compute_jaccard_similarity(clause_texts, chunk_texts):\n",
    "    \"\"\"\n",
    "    Uses CountVectorizer(binary=True) to convert texts to binary vectors\n",
    "    and then computes Jaccard similarity for each (clause, chunk) pair.\n",
    "    \"\"\"\n",
    "    if not clause_texts or not chunk_texts:\n",
    "        return np.array([])\n",
    "\n",
    "    all_texts = clause_texts + chunk_texts\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    binary_matrix = vectorizer.fit_transform(all_texts).toarray()\n",
    "    \n",
    "    clause_binary = binary_matrix[:len(clause_texts)]\n",
    "    chunk_binary = binary_matrix[len(clause_texts):]\n",
    "    \n",
    "    similarities = np.zeros((len(clause_binary), len(chunk_binary)))\n",
    "    for i, clause_vec in enumerate(clause_binary):\n",
    "        for j, chunk_vec in enumerate(chunk_binary):\n",
    "            sim = jaccard_score(clause_vec, chunk_vec, average='binary')\n",
    "            similarities[i, j] = sim\n",
    "    return similarities\n",
    "\n",
    "def check_clauses(contract_chunks, required_clauses, model, alpha=0.5, beta=0.3, gamma=0.2):\n",
    "    \"\"\"\n",
    "    Checks if the contract chunks contain the required clauses.\n",
    "    Returns a dictionary with clause names as keys and a dict\n",
    "    containing score, best chunk index, and chunk text as values.\n",
    "\n",
    "    alpha, beta, gamma are the weights for embedding similarity, TF-IDF, and Jaccard, respectively.\n",
    "    \"\"\"\n",
    "    # Extract actual text from dictionaries\n",
    "    clause_texts = [c[\"clauseText\"].strip().lower() for c in required_clauses if \"clauseText\" in c and c[\"clauseText\"].strip()]\n",
    "    chunk_texts  = [ch[\"text\"].strip().lower() for ch in contract_chunks if \"text\" in ch and ch[\"text\"].strip()]\n",
    "    \n",
    "    print(f\"\\nNumber of clauses: {len(clause_texts)}\")\n",
    "    print(f\"Number of chunks: {len(chunk_texts)}\")\n",
    "    \n",
    "    if not clause_texts:\n",
    "        print(\"No valid clause texts found.\")\n",
    "        return {}\n",
    "    if not chunk_texts:\n",
    "        print(\"No valid chunk texts found.\")\n",
    "        return {}\n",
    "    \n",
    "    # Embed chunk texts\n",
    "    chunk_embeddings = embed_text(chunk_texts, model)\n",
    "    \n",
    "    # Embed clause texts\n",
    "    clause_embeddings = embed_text(clause_texts, model)\n",
    "    \n",
    "    print(f\"Embedded {len(clause_embeddings)} clause embeddings and {len(chunk_embeddings)} chunk embeddings.\")\n",
    "    \n",
    "    # Compute TF-IDF and Jaccard similarities\n",
    "    tfidf_similarities = compute_tfidf_similarity(clause_texts, chunk_texts)\n",
    "    jaccard_similarities = compute_jaccard_similarity(clause_texts, chunk_texts)\n",
    "    \n",
    "    print(\"TF-IDF Similarities:\\n\", tfidf_similarities)\n",
    "    print(\"Jaccard Similarities:\\n\", jaccard_similarities)\n",
    "    \n",
    "    if tfidf_similarities.size == 0 or jaccard_similarities.size == 0:\n",
    "        print(\"Similarity matrices are empty.\")\n",
    "        return {}\n",
    "    \n",
    "    clause_similarities = {}\n",
    "    \n",
    "    for i, clause_emb in enumerate(clause_embeddings):\n",
    "        best_score = -1\n",
    "        best_chunk_index = None\n",
    "        \n",
    "        for j, chunk_emb in enumerate(chunk_embeddings):\n",
    "            # Cosine similarity (inverted sign to get actual similarity)\n",
    "            sim = tf.keras.losses.cosine_similarity(clause_emb, chunk_emb).numpy()\n",
    "            sim = -sim  # range goes from -1..1; we invert it to get 1 as high similarity\n",
    "            \n",
    "            # Normalize cosine similarity from -1..1 to 0..1\n",
    "            norm_sim = (sim + 1) / 2\n",
    "            \n",
    "            # Merge similarities\n",
    "            tfidf_part = tfidf_similarities[i, j] if tfidf_similarities.size else 0\n",
    "            jaccard_part = jaccard_similarities[i, j] if jaccard_similarities.size else 0\n",
    "            \n",
    "            combined_score = (\n",
    "                alpha * norm_sim +\n",
    "                beta  * tfidf_part +\n",
    "                gamma * jaccard_part\n",
    "            )\n",
    "            \n",
    "            print(f\"Clause {i}: Chunk {j} - Norm Sim: {norm_sim:.2f}, TF-IDF: {tfidf_part:.2f}, Jaccard: {jaccard_part:.2f}, Combined: {combined_score:.2f}\")\n",
    "            \n",
    "            if combined_score > best_score:\n",
    "                best_score = combined_score\n",
    "                best_chunk_index = j\n",
    "        \n",
    "        clause_name = required_clauses[i][\"clauseName\"].strip() if \"clauseName\" in required_clauses[i] else f\"Clause_{i}\"\n",
    "        chunk_text = chunk_texts[best_chunk_index] if (best_chunk_index is not None and best_chunk_index < len(chunk_texts)) else None\n",
    "        \n",
    "        clause_similarities[clause_name] = {\n",
    "            \"score\": best_score,\n",
    "            \"chunk_index\": best_chunk_index,\n",
    "            \"chunk_text\": chunk_text\n",
    "        }\n",
    "    \n",
    "    return clause_similarities\n",
    "\n",
    "def load_items(jsonl_path):\n",
    "    \"\"\"\n",
    "    Loads items from a JSONL file.\n",
    "    Each line should be a valid JSON object.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, line in enumerate(f, start=1):\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                items.append(data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line {idx}: {e}\")\n",
    "    print(f\"Loaded {len(items)} items from {jsonl_path}\")\n",
    "    if items:\n",
    "        print(f\"First item: {items[0]}\")\n",
    "    return items\n",
    "\n",
    "def display_results_as_dataframe(results):\n",
    "    \"\"\"\n",
    "    Displays the results as a formatted Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        display(Markdown(\"**No results to display.**\"))\n",
    "        return\n",
    "    \n",
    "    data = []\n",
    "    for clause_name, info in results.items():\n",
    "        data.append({\n",
    "            \"Clause Name\": clause_name,\n",
    "            \"Similarity\": f\"{info['score']:.2f}\",\n",
    "            \"Best Chunk Index\": info[\"chunk_index\"] if info[\"chunk_index\"] is not None else \"None\",\n",
    "            \"Chunk Text\": info[\"chunk_text\"] if info[\"chunk_text\"] else \"None\"\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values(by=\"Similarity\", ascending=False)\n",
    "    \n",
    "    # Display the dataframe\n",
    "    display(df)\n",
    "\n",
    "############################\n",
    "# USAGE EXAMPLE (no if __name__ guard)\n",
    "############################\n",
    "\n",
    "# 1. Initialize a SentenceTransformer model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# 2. Load clauses and contract chunks\n",
    "master_clauses = load_items(\"master_contract_clause.jsonl\")\n",
    "sample_chunks  = load_items(\"chunks.jsonl\")\n",
    "\n",
    "# 3. Check clauses\n",
    "results = check_clauses(\n",
    "    contract_chunks=sample_chunks,\n",
    "    required_clauses=master_clauses,\n",
    "    model=model,\n",
    "    alpha=0.5,  # weight for embedding similarity\n",
    "    beta=0.3,   # weight for TF-IDF similarity\n",
    "    gamma=0.2   # weight for Jaccard similarity\n",
    ")\n",
    "\n",
    "# 4. Display results\n",
    "display_results_as_dataframe(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
